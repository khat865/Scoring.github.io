import json
import random
import re
from typing import List, Dict, Tuple, Optional
import numpy as np
from datetime import datetime


def normalize_diagnosis(diag: str) -> str:
    """æ ‡å‡†åŒ–è¯Šæ–­åç§°ï¼Œç”¨äºåŒ¹é…"""
    if not diag:
        return ""
    # è½¬å°å†™ï¼Œå»é™¤æ ‡ç‚¹å’Œå¤šä½™ç©ºæ ¼
    diag = diag.lower().strip()
    diag = re.sub(r'[^\w\s]', ' ', diag)
    diag = re.sub(r'\s+', ' ', diag)
    return diag


def replace_path_prefix(image_paths: List[str]) -> List[str]:
    """
    æ›¿æ¢å›¾ç‰‡è·¯å¾„å‰ç¼€
    å°† /gpfs/radev/pi/q_chen/zq65/Research/Data/DermDPO/datasets/eval/ æ›¿æ¢ä¸º images/
    """
    old_prefix = "/gpfs/radev/pi/q_chen/zq65/Research/Data/DermDPO/datasets/eval/"
    new_prefix = "images/"
    
    replaced_paths = []
    replaced_count = 0
    for path in image_paths:
        if path.startswith(old_prefix):
            # æ›¿æ¢å‰ç¼€
            new_path = new_prefix + path[len(old_prefix):]
            replaced_paths.append(new_path)
            replaced_count += 1
        else:
            # å¦‚æœä¸æ˜¯ç›®æ ‡å‰ç¼€ï¼Œä¿æŒåŸæ ·
            replaced_paths.append(path)
    
    return replaced_paths, replaced_count


def diagnoses_match(diag1: str, diag2: str, threshold: float = 0.8) -> bool:
    """
    åˆ¤æ–­ä¸¤ä¸ªè¯Šæ–­æ˜¯å¦åŒ¹é…
    ä½¿ç”¨å¤šç§ç­–ç•¥ï¼šå®Œå…¨åŒ¹é…ã€åŒ…å«å…³ç³»ã€è¯æ±‡é‡å 
    """
    if not diag1 or not diag2:
        return False
    
    d1 = normalize_diagnosis(diag1)
    d2 = normalize_diagnosis(diag2)
    
    # å®Œå…¨åŒ¹é…
    if d1 == d2:
        return True
    
    # åŒ…å«å…³ç³»ï¼ˆé•¿åº¦ç›¸è¿‘ï¼‰
    if d1 in d2 or d2 in d1:
        len_ratio = min(len(d1), len(d2)) / max(len(d1), len(d2))
        if len_ratio > threshold:
            return True
    
    # è¯æ±‡é‡å åº¦
    words1 = set(d1.split())
    words2 = set(d2.split())
    if words1 and words2:
        overlap = len(words1.intersection(words2)) / len(words1.union(words2))
        if overlap > threshold:
            return True
    
    return False


def match_case_by_gt(case_eval: Dict, medical_data: List[Dict], debug=False) -> Optional[Dict]:
    """
    é€šè¿‡ground_truthåŒ¹é…ç—…ä¾‹
    è¿”å›åŒ¹é…åˆ°çš„medical_dataæ¡ç›®
    """
    eval_gt_list = case_eval.get('ground_truth_differential', [])
    if not eval_gt_list:
        return None
    
    # æ ‡å‡†åŒ–è¯„ä¼°æ–‡ä»¶çš„GT
    eval_gt_normalized = [normalize_diagnosis(gt) for gt in eval_gt_list]
    
    best_match = None
    best_score = 0
    
    for med_item in medical_data:
        # æå–medical_dataçš„GT
        med_gt = med_item.get('GT', {})
        med_gt_list = [v for k, v in med_gt.items() if k.startswith('label_')]
        
        if not med_gt_list:
            continue
        
        # æ ‡å‡†åŒ–medical_dataçš„GT
        med_gt_normalized = [normalize_diagnosis(gt) for gt in med_gt_list]
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°
        match_count = 0
        for eval_gt in eval_gt_normalized:
            for med_gt in med_gt_normalized:
                if diagnoses_match(eval_gt, med_gt):
                    match_count += 1
                    break
        
        # è®¡ç®—åŒ¹é…ç‡
        match_score = match_count / max(len(eval_gt_normalized), len(med_gt_normalized))
        
        if match_score > best_score:
            best_score = match_score
            best_match = med_item
    
    # åªæœ‰åŒ¹é…åº¦è¶…è¿‡é˜ˆå€¼æ‰è¿”å›
    if best_score >= 0.5:
        return best_match
    
    return None


def calculate_similarity_variance(task3_pairs: List[Dict]) -> float:
    """
    è®¡ç®—task3_pairsä¸­ç›¸ä¼¼åº¦çš„æ³¢åŠ¨ç¨‹åº¦
    è¿”å›æ ‡å‡†å·®ï¼Œæ³¢åŠ¨è¶Šå¤§å€¼è¶Šé«˜
    """
    if not task3_pairs or len(task3_pairs) < 2:
        return 0.0
    
    similarities = [pair.get('similarity', 0.0) for pair in task3_pairs]
    
    # è®¡ç®—æ ‡å‡†å·®
    return float(np.std(similarities))


def calculate_similarity_range(task3_pairs: List[Dict]) -> float:
    """
    è®¡ç®—task3_pairsä¸­ç›¸ä¼¼åº¦çš„æå·®ï¼ˆæœ€å¤§å€¼-æœ€å°å€¼ï¼‰
    """
    if not task3_pairs or len(task3_pairs) < 2:
        return 0.0
    
    similarities = [pair.get('similarity', 0.0) for pair in task3_pairs]
    return max(similarities) - min(similarities)


def get_dermlip_similarity(similarity_matrix, pred_idx: int, truth_idx: int) -> float:
    """ä»ç›¸ä¼¼åº¦çŸ©é˜µä¸­è·å–ç›¸ä¼¼åº¦åˆ†æ•°"""
    if similarity_matrix and len(similarity_matrix) > pred_idx:
        if len(similarity_matrix[pred_idx]) > truth_idx:
            return similarity_matrix[pred_idx][truth_idx]
    return 0.0


def calculate_jaccard_similarity(diag1: str, diag2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªè¯Šæ–­ä¹‹é—´çš„Jaccardç›¸ä¼¼åº¦"""
    words1 = set(normalize_diagnosis(diag1).split())
    words2 = set(normalize_diagnosis(diag2).split())
    
    if len(words1) == 0 and len(words2) == 0:
        return 0.0
    
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return intersection / union if union > 0 else 0.0


def generate_task3_pairs(
    pred_diff_list: List[str],
    truth_diff_list: List[str],
    similarity_matrix: Optional[List[List[float]]] = None
) -> List[Dict]:
    """
    ç”Ÿæˆtask3çš„ä¸¤å¯¹è¯Šæ–­ï¼Œä¼˜å…ˆé€‰æ‹©ç›¸ä¼¼åº¦å·®å¼‚å¤§çš„å¯¹
    """
    if len(pred_diff_list) < 2 or len(truth_diff_list) < 2:
        # æ•°é‡ä¸è¶³ï¼Œåˆ›å»ºé»˜è®¤å¯¹
        return create_default_pairs(pred_diff_list, truth_diff_list, similarity_matrix)
    
    # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„é…å¯¹åŠå…¶ç›¸ä¼¼åº¦
    all_pairs = []
    for i, pred in enumerate(pred_diff_list):
        for j, truth in enumerate(truth_diff_list):
            if similarity_matrix and i < len(similarity_matrix) and j < len(similarity_matrix[i]):
                similarity = similarity_matrix[i][j]
            else:
                similarity = calculate_jaccard_similarity(pred, truth)
            
            all_pairs.append({
                'pred_idx': i,
                'truth_idx': j,
                'predicted': pred,
                'ground_truth': truth,
                'similarity': round(similarity, 4)
            })
    
    # ç­–ç•¥ï¼šé€‰æ‹©ç›¸ä¼¼åº¦å·®å¼‚æœ€å¤§çš„ä¸¤å¯¹
    # å°è¯•æ‰€æœ‰ä¸¤å¯¹ç»„åˆï¼Œæ‰¾å‡ºç›¸ä¼¼åº¦å·®å¼‚æœ€å¤§çš„
    best_variance = -1
    best_pair_combo = None
    
    for i in range(len(all_pairs)):
        for j in range(i + 1, len(all_pairs)):
            pair1 = all_pairs[i]
            pair2 = all_pairs[j]
            
            # ç¡®ä¿ä¸é‡å¤ä½¿ç”¨åŒä¸€ä¸ªé¢„æµ‹æˆ–çœŸå®è¯Šæ–­
            if (pair1['pred_idx'] == pair2['pred_idx'] or 
                pair1['truth_idx'] == pair2['truth_idx']):
                continue
            
            # è®¡ç®—ç›¸ä¼¼åº¦å·®å¼‚
            sim_diff = abs(pair1['similarity'] - pair2['similarity'])
            
            if sim_diff > best_variance:
                best_variance = sim_diff
                best_pair_combo = (pair1, pair2)
    
    if best_pair_combo:
        pair1, pair2 = best_pair_combo
        return [
            {
                'pair_id': 'A',
                'predicted': pair1['predicted'],
                'ground_truth': pair1['ground_truth'],
                'similarity': pair1['similarity']
            },
            {
                'pair_id': 'B',
                'predicted': pair2['predicted'],
                'ground_truth': pair2['ground_truth'],
                'similarity': pair2['similarity']
            }
        ]
    else:
        # é™çº§ï¼šéšæœºé€‰æ‹©ä¸¤å¯¹
        return create_default_pairs(pred_diff_list, truth_diff_list, similarity_matrix)


def create_default_pairs(
    pred_diff_list: List[str],
    truth_diff_list: List[str],
    similarity_matrix: Optional[List[List[float]]] = None
) -> List[Dict]:
    """åˆ›å»ºé»˜è®¤çš„ä¸¤å¯¹è¯Šæ–­"""
    pairs = []
    
    for i in range(2):
        pred_idx = min(i, len(pred_diff_list) - 1) if pred_diff_list else 0
        truth_idx = min(i, len(truth_diff_list) - 1) if truth_diff_list else 0
        
        pred = pred_diff_list[pred_idx] if pred_diff_list else "æ— é¢„æµ‹è¯Šæ–­"
        truth = truth_diff_list[truth_idx] if truth_diff_list else "æ— çœŸå®è¯Šæ–­"
        
        if similarity_matrix and pred_idx < len(similarity_matrix) and truth_idx < len(similarity_matrix[pred_idx]):
            similarity = similarity_matrix[pred_idx][truth_idx]
        else:
            similarity = calculate_jaccard_similarity(pred, truth)
        
        pairs.append({
            'pair_id': chr(65 + i),
            'predicted': pred,
            'ground_truth': truth,
            'similarity': round(similarity, 4)
        })
    
    return pairs


def process_evaluation_data(
    eval_file: str,
    medical_file: str,
    output_file: str,
    use_dermlip: bool = True
) -> List[Dict]:
    """
    å¤„ç†è¯„ä¼°æ•°æ®ï¼ŒåŒ¹é…å›¾ç‰‡è·¯å¾„ï¼Œç”Ÿæˆæ ‡å‡†æ ¼å¼
    """
    print(f"è¯»å–è¯„ä¼°æ•°æ®: {eval_file}")
    with open(eval_file, 'r', encoding='utf-8') as f:
        eval_data = json.load(f)
    
    print(f"è¯»å–åŒ»ç–—æ•°æ®: {medical_file}")
    with open(medical_file, 'r', encoding='utf-8') as f:
        medical_data = json.load(f)
    
    print(f"è¯„ä¼°æ•°æ®: {len(eval_data.get('per_sample_results', []))} ä¸ªç—…ä¾‹")
    print(f"åŒ»ç–—æ•°æ®: {len(medical_data)} ä¸ªç—…ä¾‹")
    
    # ç»Ÿè®¡å˜é‡
    processed_cases = []
    matched_count = 0
    fallback_count = 0
    no_image_count = 0
    total_path_replaced = 0
    
    per_sample_results = eval_data.get('per_sample_results', [])
    
    print(f"\n{'='*70}")
    print(f"å¼€å§‹å¤„ç†ç—…ä¾‹ï¼ˆå…± {len(per_sample_results)} ä¸ªï¼‰...")
    print(f"{'='*70}")
    
    start_time = datetime.now()
    
    for idx, sample in enumerate(per_sample_results):
        # æ¯å¤„ç†500ä¸ªç—…ä¾‹æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if (idx + 1) % 500 == 0 or idx == 0:
            elapsed = (datetime.now() - start_time).total_seconds()
            speed = (idx + 1) / elapsed if elapsed > 0 else 0
            eta = (len(per_sample_results) - idx - 1) / speed if speed > 0 else 0
            
            print(f"\nè¿›åº¦: {idx + 1}/{len(per_sample_results)} ({(idx+1)/len(per_sample_results)*100:.1f}%)")
            print(f"  â”œâ”€ GTåŒ¹é…æˆåŠŸ: {matched_count}")
            print(f"  â”œâ”€ é¡ºåºåŒ¹é…: {fallback_count}")
            print(f"  â”œâ”€ æ— å›¾ç‰‡: {no_image_count}")
            print(f"  â”œâ”€ è·¯å¾„å·²æ›¿æ¢: {total_path_replaced}")
            print(f"  â”œâ”€ å¤„ç†é€Ÿåº¦: {speed:.1f} ä¸ª/ç§’")
            print(f"  â””â”€ é¢„è®¡å‰©ä½™æ—¶é—´: {eta/60:.1f} åˆ†é’Ÿ")
        
        case_id = sample.get('id', f'case_{idx}')
        
        # æå–é¢„æµ‹å’ŒçœŸå®çš„é‰´åˆ«è¯Šæ–­
        pred_diff = sample.get('predicted_differential', [])
        truth_diff = sample.get('ground_truth_differential', [])
        
        if not isinstance(pred_diff, list):
            pred_diff = []
        if not isinstance(truth_diff, list):
            truth_diff = []
        
        # è·å–ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = None
        if use_dermlip:
            dermlip_metrics = sample.get('dermlip_metrics', {})
            similarity_matrix = dermlip_metrics.get('similarity_matrix', None)
        
        # åŒ¹é…medical_dataä»¥è·å–image_pathså’Œprompt
        matched_item = match_case_by_gt(sample, medical_data)
        
        if matched_item:
            matched_count += 1
            image_paths = matched_item.get('image_paths', [])
            prompt = matched_item.get('prompt', '')
        else:
            # å°è¯•æŒ‰é¡ºåºåŒ¹é…ï¼ˆä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰
            fallback_count += 1
            if idx < len(medical_data):
                image_paths = medical_data[idx].get('image_paths', [])
                prompt = medical_data[idx].get('prompt', '')
            else:
                image_paths = []
                prompt = ''
        
        # ç»Ÿè®¡æ— å›¾ç‰‡çš„ç—…ä¾‹
        if not image_paths or len(image_paths) == 0:
            no_image_count += 1
        
        # æ›¿æ¢è·¯å¾„å‰ç¼€
        image_paths, replaced_count = replace_path_prefix(image_paths)
        total_path_replaced += replaced_count
        
        # ç”Ÿæˆtask3_pairsï¼šé€‰æ‹©ç›¸ä¼¼åº¦æ³¢åŠ¨å¤§çš„ä¸¤å¯¹
        task3_pairs = generate_task3_pairs(
            pred_diff, 
            truth_diff, 
            similarity_matrix
        )
        
        # æ„å»ºç—…ä¾‹æ•°æ®
        case_data = {
            "id": case_id,
            "pmid": case_id,
            
            # ä»»åŠ¡1ï¼šå›¾ç‰‡è·¯å¾„å’Œæç¤ºè¯
            "image_paths": image_paths,
            "prompt": prompt,
            
            # ä»»åŠ¡2ï¼šè¯Šæ–­
            "predicted_diagnosis": sample.get('predicted_differential_text', ''),
            "ground_truth_diagnosis": ' | '.join(truth_diff) if truth_diff else '',
            
            # ä»»åŠ¡3ï¼šä¸¤å¯¹è¯Šæ–­åŠç›¸ä¼¼åº¦
            "task3_pairs": task3_pairs,
            
            # é¢å¤–ä¿¡æ¯
            "predicted_differential_diagnosis_full": pred_diff,
            "ground_truth_differential_diagnosis_full": truth_diff,
            
            # ç›¸ä¼¼åº¦æ³¢åŠ¨æŒ‡æ ‡
            "similarity_variance": calculate_similarity_variance(task3_pairs),
            "similarity_range": calculate_similarity_range(task3_pairs)
        }
        
        processed_cases.append(case_data)
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*70}")
    print(f"æ­¥éª¤1å¤„ç†å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"æ€»å¤„ç†ç—…ä¾‹: {len(processed_cases)}")
    print(f"GTåŒ¹é…æˆåŠŸ: {matched_count} ({matched_count/len(processed_cases)*100:.1f}%)")
    print(f"é¡ºåºåŒ¹é…: {fallback_count} ({fallback_count/len(processed_cases)*100:.1f}%)")
    print(f"æ— å›¾ç‰‡è·¯å¾„: {no_image_count} ({no_image_count/len(processed_cases)*100:.1f}%)")
    print(f"è·¯å¾„å·²æ›¿æ¢: {total_path_replaced}")
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(processed_cases, f, ensure_ascii=False, indent=2)
    
    print(f"\nä¸´æ—¶æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    return processed_cases


def validate_and_filter_data(
    input_file: str,
    output_file: str,
    max_cases: int = 50,
    min_similarity_variance: float = 0.0
) -> Tuple[List[Dict], List[Dict]]:
    """
    éªŒè¯å¹¶ç­›é€‰æ•°æ®ï¼Œä¼˜å…ˆä¿ç•™ç›¸ä¼¼åº¦æ³¢åŠ¨å¤§çš„ç—…ä¾‹
    """
    print(f"\n{'='*70}")
    print("æ­¥éª¤2: å¼€å§‹æ•°æ®ç­›é€‰å’ŒéªŒè¯")
    print(f"{'='*70}")
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"è¯»å–ä¸´æ—¶æ•°æ®: {len(data)} ä¸ªç—…ä¾‹")
    
    valid_cases = []
    removed_cases = []
    
    # ç»Ÿè®¡å„ç§ç§»é™¤åŸå› 
    reason_stats = {
        'åŒ…å«Noneå€¼': 0,
        'task3_pairsé—®é¢˜': 0,
        'è¯Šæ–­ä¸ºç©º': 0,
        'è¯Šæ–­ä¸€è‡´': 0,
        'æ— å›¾ç‰‡è·¯å¾„': 0,
        'ç¼ºå°‘å¿…éœ€å­—æ®µ': 0,
        'ç›¸ä¼¼åº¦æ³¢åŠ¨è¿‡å°': 0
    }
    
    print(f"\nå¼€å§‹éªŒè¯...")
    
    for idx, case in enumerate(data):
        if (idx + 1) % 1000 == 0:
            print(f"  éªŒè¯è¿›åº¦: {idx + 1}/{len(data)} ({(idx+1)/len(data)*100:.1f}%)")
        
        case_id = case.get('pmid', case.get('id', f'case_{idx}'))
        remove_reasons = []
        
        # è§„åˆ™1: æ£€æŸ¥æ˜¯å¦æœ‰Noneå€¼
        if has_none_values(case):
            remove_reasons.append("åŒ…å«Noneå€¼")
            reason_stats['åŒ…å«Noneå€¼'] += 1
        
        # è§„åˆ™2: æ£€æŸ¥task3_pairs
        task3_valid, task3_reason = validate_task3_pairs(case.get('task3_pairs', []))
        if not task3_valid:
            remove_reasons.append(task3_reason)
            reason_stats['task3_pairsé—®é¢˜'] += 1
        
        # è§„åˆ™3: æ£€æŸ¥è¯Šæ–­æ˜¯å¦ä¸€è‡´
        pred_diag = case.get('predicted_diagnosis', '')
        truth_diag = case.get('ground_truth_diagnosis', '')
        
        if not pred_diag or not truth_diag:
            remove_reasons.append("è¯Šæ–­ä¸ºç©º")
            reason_stats['è¯Šæ–­ä¸ºç©º'] += 1
        elif diagnoses_are_same(pred_diag, truth_diag):
            remove_reasons.append(f"è¯Šæ–­å®Œå…¨ä¸€è‡´")
            reason_stats['è¯Šæ–­ä¸€è‡´'] += 1
        
        # è§„åˆ™4: æ£€æŸ¥å›¾ç‰‡è·¯å¾„
        image_paths = case.get('image_paths', [])
        if not image_paths or len(image_paths) == 0:
            remove_reasons.append("æ— å›¾ç‰‡è·¯å¾„")
            reason_stats['æ— å›¾ç‰‡è·¯å¾„'] += 1
        
        # è§„åˆ™5: æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['id', 'pmid', 'prompt']
        for field in required_fields:
            if not case.get(field):
                remove_reasons.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                reason_stats['ç¼ºå°‘å¿…éœ€å­—æ®µ'] += 1
                break
        
        # è§„åˆ™6: æ£€æŸ¥ç›¸ä¼¼åº¦æ³¢åŠ¨
        sim_variance = case.get('similarity_variance', 0.0)
        if sim_variance < min_similarity_variance:
            remove_reasons.append(f"ç›¸ä¼¼åº¦æ³¢åŠ¨è¿‡å°: {sim_variance:.4f} < {min_similarity_variance}")
            reason_stats['ç›¸ä¼¼åº¦æ³¢åŠ¨è¿‡å°'] += 1
        
        if remove_reasons:
            removed_cases.append({
                'case_id': case_id,
                'reasons': remove_reasons,
                'predicted_diagnosis': pred_diag,
                'ground_truth_diagnosis': truth_diag,
                'similarity_variance': case.get('similarity_variance', 0.0)
            })
        else:
            valid_cases.append(case)
    
    print(f"\nåŸºæœ¬éªŒè¯å®Œæˆ:")
    print(f"  é€šè¿‡éªŒè¯: {len(valid_cases)} ä¸ª")
    print(f"  æœªé€šè¿‡éªŒè¯: {len(removed_cases)} ä¸ª")
    
    print(f"\næœªé€šè¿‡åŸå› ç»Ÿè®¡:")
    for reason, count in sorted(reason_stats.items(), key=lambda x: x[1], reverse=True):
        if count > 0:
            print(f"  â€¢ {reason}: {count} ä¸ª")
    
    # æŒ‰ç›¸ä¼¼åº¦æ³¢åŠ¨æ’åºï¼Œé€‰æ‹©æ³¢åŠ¨æœ€å¤§çš„ç—…ä¾‹
    if len(valid_cases) > max_cases:
        print(f"\n{'='*70}")
        print(f"æœ‰æ•ˆç—…ä¾‹æ•°({len(valid_cases)})è¶…è¿‡ç›®æ ‡æ•°({max_cases})ï¼Œå¼€å§‹æŒ‰ç›¸ä¼¼åº¦æ³¢åŠ¨æ’åº...")
        print(f"{'='*70}")
        
        # æŒ‰ç›¸ä¼¼åº¦æ³¢åŠ¨ï¼ˆæ ‡å‡†å·®å’Œæå·®çš„ç»„åˆï¼‰é™åºæ’åº
        valid_cases.sort(
            key=lambda x: (
                x.get('similarity_variance', 0) * 0.6 + 
                x.get('similarity_range', 0) * 0.4
            ),
            reverse=True
        )
        
        # æ˜¾ç¤ºæ³¢åŠ¨åˆ†å¸ƒ
        print(f"\nç›¸ä¼¼åº¦æ³¢åŠ¨åˆ†æ:")
        print(f"  å‰10ä¸ªç—…ä¾‹çš„æ–¹å·®: {[round(c.get('similarity_variance', 0), 4) for c in valid_cases[:10]]}")
        print(f"  å10ä¸ªç—…ä¾‹çš„æ–¹å·®: {[round(c.get('similarity_variance', 0), 4) for c in valid_cases[-10:]]}")
        
        # ä¿ç•™å‰max_casesä¸ª
        extra_cases = valid_cases[max_cases:]
        valid_cases = valid_cases[:max_cases]
        
        print(f"\nä¿ç•™å‰{max_cases}ä¸ªæ³¢åŠ¨æœ€å¤§çš„ç—…ä¾‹")
        print(f"é¢å¤–ç§»é™¤: {len(extra_cases)} ä¸ªç—…ä¾‹ï¼ˆç›¸ä¼¼åº¦æ³¢åŠ¨è¾ƒå°ï¼‰")
        
        # å°†å¤šä½™çš„ç—…ä¾‹åŠ å…¥ç§»é™¤åˆ—è¡¨
        for case in extra_cases:
            removed_cases.append({
                'case_id': case.get('pmid', case.get('id')),
                'reasons': [f'ç›¸ä¼¼åº¦æ³¢åŠ¨è¾ƒå°ï¼Œæœªè¿›å…¥å‰{max_cases}å'],
                'predicted_diagnosis': case.get('predicted_diagnosis', ''),
                'ground_truth_diagnosis': case.get('ground_truth_diagnosis', ''),
                'similarity_variance': case.get('similarity_variance', 0.0)
            })
    
    # ä¿å­˜æœ‰æ•ˆæ•°æ®
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(valid_cases, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*70}")
    print("ç­›é€‰å®Œæˆï¼æœ€ç»ˆç»Ÿè®¡")
    print(f"{'='*70}")
    print(f"åŸå§‹ç—…ä¾‹æ•°: {len(data)}")
    print(f"âœ“ æœ€ç»ˆä¿ç•™: {len(valid_cases)} ä¸ª")
    print(f"âœ— ç§»é™¤æ€»æ•°: {len(removed_cases)} ä¸ª")
    print(f"ä¿ç•™æ¯”ä¾‹: {len(valid_cases)/len(data)*100:.2f}%")
    
    # ç»Ÿè®¡æœ€ç»ˆä¿ç•™ç—…ä¾‹çš„ç›¸ä¼¼åº¦æ³¢åŠ¨
    if valid_cases:
        variances = [c.get('similarity_variance', 0) for c in valid_cases]
        ranges = [c.get('similarity_range', 0) for c in valid_cases]
        
        print(f"\nä¿ç•™ç—…ä¾‹çš„ç›¸ä¼¼åº¦æ³¢åŠ¨ç»Ÿè®¡:")
        print(f"  æ–¹å·®ç»Ÿè®¡:")
        print(f"    å¹³å‡å€¼: {np.mean(variances):.4f}")
        print(f"    ä¸­ä½æ•°: {np.median(variances):.4f}")
        print(f"    æœ€å¤§å€¼: {np.max(variances):.4f}")
        print(f"    æœ€å°å€¼: {np.min(variances):.4f}")
        print(f"  æå·®ç»Ÿè®¡:")
        print(f"    å¹³å‡å€¼: {np.mean(ranges):.4f}")
        print(f"    ä¸­ä½æ•°: {np.median(ranges):.4f}")
        print(f"    æœ€å¤§å€¼: {np.max(ranges):.4f}")
        print(f"    æœ€å°å€¼: {np.min(ranges):.4f}")
    
    print(f"\nâœ“ æœ€ç»ˆæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    print(f"{'='*70}\n")
    
    return valid_cases, removed_cases


def has_none_values(obj, path="") -> bool:
    """é€’å½’æ£€æŸ¥å¯¹è±¡ä¸­æ˜¯å¦æœ‰Noneå€¼"""
    if obj is None:
        return True
    
    if isinstance(obj, dict):
        for key, value in obj.items():
            if has_none_values(value, f"{path}.{key}"):
                return True
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            if has_none_values(item, f"{path}[{i}]"):
                return True
    
    return False


def validate_task3_pairs(pairs: List[Dict]) -> Tuple[bool, str]:
    """éªŒè¯task3_pairs"""
    if not pairs or len(pairs) != 2:
        return False, f"task3_pairsæ•°é‡å¿…é¡»ä¸º2ä¸ª (å½“å‰: {len(pairs) if pairs else 0})"
    
    for i, pair in enumerate(pairs):
        if not isinstance(pair, dict):
            return False, f"task3_pairs[{i}]ä¸æ˜¯å­—å…¸"
        
        required_fields = ['pair_id', 'predicted', 'ground_truth', 'similarity']
        for field in required_fields:
            if field not in pair:
                return False, f"task3_pairs[{i}]ç¼ºå°‘å­—æ®µ: {field}"
            if pair[field] is None:
                return False, f"task3_pairs[{i}].{field}ä¸ºNone"
    
    pair_a, pair_b = pairs[0], pairs[1]
    
    if pair_a.get('pair_id') == pair_b.get('pair_id'):
        return False, "ä¸¤å¯¹çš„pair_idç›¸åŒ"
    
    if (pair_a.get('predicted') == pair_b.get('predicted') and 
        pair_a.get('ground_truth') == pair_b.get('ground_truth')):
        return False, "ä¸¤å¯¹çš„è¯Šæ–­å†…å®¹å®Œå…¨ç›¸åŒ"
    
    return True, ""


def diagnoses_are_same(diag1: str, diag2: str) -> bool:
    """åˆ¤æ–­ä¸¤ä¸ªè¯Šæ–­æ˜¯å¦ç›¸åŒ"""
    if not diag1 or not diag2:
        return False
    
    d1 = normalize_diagnosis(diag1)
    d2 = normalize_diagnosis(diag2)
    
    if d1 == d2:
        return True
    
    # æ£€æŸ¥åŒ…å«å…³ç³»
    if d1 in d2 or d2 in d1:
        len_ratio = min(len(d1), len(d2)) / max(len(d1), len(d2))
        if len_ratio > 0.8:
            return True
    
    return False


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print(" " * 20 + "åŒ»ç–—æ•°æ®å¤„ç†å’Œç­›é€‰è„šæœ¬")
    print("="*70)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    eval_file = "scin_dd_evaluation.json"
    medical_file = "SCIN_DD_input.json"
    temp_output = "data_temp.json"
    final_output = "data_filtered.json"
    
    # æ­¥éª¤1: å¤„ç†å’ŒåŒ¹é…æ•°æ®
    print("\nğŸ“‹ æ­¥éª¤1: å¤„ç†è¯„ä¼°æ•°æ®å¹¶åŒ¹é…å›¾ç‰‡è·¯å¾„")
    print("-" * 70)
    try:
        processed_data = process_evaluation_data(
            eval_file=eval_file,
            medical_file=medical_file,
            output_file=temp_output,
            use_dermlip=True
        )
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: æ­¥éª¤1å¤„ç†å¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        return
    
    # æ­¥éª¤2: ç­›é€‰æ•°æ®
    print("\nğŸ“Š æ­¥éª¤2: ç­›é€‰æ•°æ®ï¼ˆä¼˜å…ˆä¿ç•™ç›¸ä¼¼åº¦æ³¢åŠ¨å¤§çš„ç—…ä¾‹ï¼‰")
    print("-" * 70)
    try:
        valid_cases, removed_cases = validate_and_filter_data(
            input_file=temp_output,
            output_file=final_output,
            max_cases=50,
            min_similarity_variance=0.0
        )
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: æ­¥éª¤2ç­›é€‰å¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        return
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if valid_cases:
        print("="*70)
        print("âœ“ å¤„ç†å®Œæˆï¼ä»¥ä¸‹æ˜¯ç¬¬ä¸€ä¸ªç—…ä¾‹ç¤ºä¾‹:")
        print("="*70)
        example = valid_cases[0]
        print(f"\nç—…ä¾‹ID: {example.get('id')}")
        print(f"å›¾ç‰‡æ•°é‡: {len(example.get('image_paths', []))}")
        if example.get('image_paths'):
            print(f"å›¾ç‰‡è·¯å¾„ç¤ºä¾‹: {example.get('image_paths')[0]}")
        print(f"é¢„æµ‹è¯Šæ–­: {example.get('predicted_diagnosis', '')[:80]}...")
        print(f"çœŸå®è¯Šæ–­: {example.get('ground_truth_diagnosis', '')[:80]}...")
        print(f"ç›¸ä¼¼åº¦æ–¹å·®: {example.get('similarity_variance', 0):.4f}")
        print(f"ç›¸ä¼¼åº¦æå·®: {example.get('similarity_range', 0):.4f}")
        print("\nTask3é…å¯¹:")
        for pair in example.get('task3_pairs', []):
            print(f"  {pair['pair_id']}å¯¹:")
            print(f"    é¢„æµ‹: {pair['predicted']}")
            print(f"    çœŸå®: {pair['ground_truth']}")
            print(f"    ç›¸ä¼¼åº¦: {pair['similarity']}")
    
    print("\n" + "="*70)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    print("\nâœ“ æ‰€æœ‰å¤„ç†å®Œæˆï¼")
    print(f"âœ“ æœ€ç»ˆæ•°æ®ä¿å­˜åœ¨: {final_output}")
    print(f"âœ“ ä¸´æ—¶æ•°æ®ä¿å­˜åœ¨: {temp_output}")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()