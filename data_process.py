import json
import random
import re
import os
from typing import List, Dict, Tuple, Optional
import numpy as np
from datetime import datetime


def normalize_diagnosis(diag: str) -> str:
    """æ ‡å‡†åŒ–è¯Šæ–­åç§°ï¼Œç”¨äºåŒ¹é…"""
    if not diag:
        return ""
    # è½¬å°å†™ï¼Œå»é™¤æ ‡ç‚¹å’Œå¤šä½™ç©ºæ ¼
    diag = diag.lower().strip()
    diag = re.sub(r'[^\w\s]', ' ', diag)
    diag = re.sub(r'\s+', ' ', diag)
    return diag


def replace_path_prefix(image_paths: List[str]) -> List[str]:
    """
    æ›¿æ¢å›¾ç‰‡è·¯å¾„å‰ç¼€
    å°† /gpfs/radev/pi/q_chen/zq65/Research/Data/DermDPO/datasets/eval/ æ›¿æ¢ä¸º images/
    """
    old_prefix = "/gpfs/radev/pi/q_chen/zq65/Research/Data/DermDPO/datasets/eval/"
    new_prefix = "images/"
    
    replaced_paths = []
    replaced_count = 0
    for path in image_paths:
        if path.startswith(old_prefix):
            # æ›¿æ¢å‰ç¼€
            new_path = new_prefix + path[len(old_prefix):]
            replaced_paths.append(new_path)
            replaced_count += 1
        else:
            # å¦‚æœä¸æ˜¯ç›®æ ‡å‰ç¼€ï¼Œä¿æŒåŸæ ·
            replaced_paths.append(path)
    
    return replaced_paths, replaced_count


def diagnoses_match(diag1: str, diag2: str, threshold: float = 0.8) -> bool:
    """
    åˆ¤æ–­ä¸¤ä¸ªè¯Šæ–­æ˜¯å¦åŒ¹é…
    ä½¿ç”¨å¤šç§ç­–ç•¥:å®Œå…¨åŒ¹é…ã€åŒ…å«å…³ç³»ã€è¯æ±‡é‡å 
    """
    if not diag1 or not diag2:
        return False
    
    d1 = normalize_diagnosis(diag1)
    d2 = normalize_diagnosis(diag2)
    
    # å®Œå…¨åŒ¹é…
    if d1 == d2:
        return True
    
    # åŒ…å«å…³ç³»(é•¿åº¦ç›¸è¿‘)
    if d1 in d2 or d2 in d1:
        len_ratio = min(len(d1), len(d2)) / max(len(d1), len(d2))
        if len_ratio > threshold:
            return True
    
    # è¯æ±‡é‡å åº¦
    words1 = set(d1.split())
    words2 = set(d2.split())
    if words1 and words2:
        overlap = len(words1.intersection(words2)) / len(words1.union(words2))
        if overlap > threshold:
            return True
    
    return False


def are_diagnoses_same(diag1: str, diag2: str, similarity_threshold: float = 0.9) -> bool:
    """
    åˆ¤æ–­ä¸¤ä¸ªè¯Šæ–­æ˜¯å¦å®è´¨ä¸Šç›¸åŒ
    ç”¨äºè¿‡æ»¤æ‰ predicted å’Œ ground_truth ç›¸åŒçš„é…å¯¹
    """
    if not diag1 or not diag2:
        return False
    
    d1 = normalize_diagnosis(diag1)
    d2 = normalize_diagnosis(diag2)
    
    # å®Œå…¨ç›¸åŒ
    if d1 == d2:
        return True
    
    # è®¡ç®— Jaccard ç›¸ä¼¼åº¦
    words1 = set(d1.split())
    words2 = set(d2.split())
    
    if not words1 or not words2:
        return False
    
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    jaccard = intersection / union if union > 0 else 0.0
    
    # å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œè®¤ä¸ºæ˜¯ç›¸åŒçš„
    if jaccard >= similarity_threshold:
        return True
    
    # æ£€æŸ¥åŒ…å«å…³ç³»ï¼ˆå‡ ä¹å®Œå…¨åŒ…å«ï¼‰
    if d1 in d2 or d2 in d1:
        len_ratio = min(len(d1), len(d2)) / max(len(d1), len(d2))
        if len_ratio > similarity_threshold:
            return True
    
    return False


def has_high_content_overlap(diag1: str, diag2: str, overlap_threshold: float = 0.7) -> bool:
    """
    æ£€æŸ¥ä¸¤ä¸ªè¯Šæ–­çš„å†…å®¹é‡å¤åº¦æ˜¯å¦è¿‡é«˜
    ä¾‹å¦‚: "contact dermatitis" vs "allergic contact dermatitis" é‡å¤åº¦å¾ˆé«˜
    
    Args:
        diag1: è¯Šæ–­1
        diag2: è¯Šæ–­2
        overlap_threshold: é‡å¤åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºé‡å¤åº¦è¿‡é«˜
    
    Returns:
        True å¦‚æœå†…å®¹é‡å¤åº¦è¿‡é«˜
    """
    if not diag1 or not diag2:
        return False
    
    d1 = normalize_diagnosis(diag1)
    d2 = normalize_diagnosis(diag2)
    
    words1 = set(d1.split())
    words2 = set(d2.split())
    
    if not words1 or not words2:
        return False
    
    # è®¡ç®— Jaccard ç›¸ä¼¼åº¦
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    jaccard = intersection / union if union > 0 else 0.0
    
    # æ£€æŸ¥é‡å¤åº¦
    if jaccard >= overlap_threshold:
        return True
    
    # æ£€æŸ¥åŒ…å«å…³ç³»ï¼šå¦‚æœä¸€ä¸ªè¯Šæ–­çš„è¯æ±‡è¢«å¦ä¸€ä¸ªå‡ ä¹å®Œå…¨åŒ…å«
    if words1 and words2:
        # å°é›†åˆåœ¨å¤§é›†åˆä¸­çš„å æ¯”
        smaller = words1 if len(words1) <= len(words2) else words2
        larger = words2 if len(words1) <= len(words2) else words1
        
        overlap_ratio = len(smaller.intersection(larger)) / len(smaller)
        if overlap_ratio >= overlap_threshold:
            return True
    
    return False


def normalize_similarities_in_case(all_pairs: List[Dict]) -> List[Dict]:
    """
    å¯¹å•ä¸ªç—…ä¾‹ä¸­çš„æ‰€æœ‰é…å¯¹çš„ç›¸ä¼¼åº¦è¿›è¡Œå½’ä¸€åŒ– (0-1)
    
    Args:
        all_pairs: æ‰€æœ‰å¯èƒ½çš„é…å¯¹åˆ—è¡¨
    
    Returns:
        å½’ä¸€åŒ–åçš„é…å¯¹åˆ—è¡¨
    """
    if not all_pairs:
        return all_pairs
    
    # æå–æ‰€æœ‰ç›¸ä¼¼åº¦å€¼
    similarities = [pair['similarity'] for pair in all_pairs]
    
    if not similarities:
        return all_pairs
    
    min_sim = min(similarities)
    max_sim = max(similarities)
    
    # å¦‚æœæœ€å¤§å€¼å’Œæœ€å°å€¼ç›¸åŒï¼Œæ‰€æœ‰ç›¸ä¼¼åº¦éƒ½è®¾ä¸º 0.5
    if max_sim - min_sim < 1e-6:
        for pair in all_pairs:
            pair['similarity_normalized'] = 0.5
            pair['similarity_original'] = pair['similarity']
        return all_pairs
    
    # Min-Max å½’ä¸€åŒ–åˆ° [0, 1]
    for pair in all_pairs:
        original_sim = pair['similarity']
        normalized_sim = (original_sim - min_sim) / (max_sim - min_sim)
        pair['similarity_normalized'] = round(normalized_sim, 4)
        pair['similarity_original'] = original_sim
        pair['similarity'] = pair['similarity_normalized']  # ä½¿ç”¨å½’ä¸€åŒ–åçš„å€¼
    
    return all_pairs


def match_case_by_gt(case_eval: Dict, medical_data: List[Dict], debug=False) -> Optional[Dict]:
    """
    é€šè¿‡ground_truthåŒ¹é…ç—…ä¾‹
    è¿”å›åŒ¹é…åˆ°çš„medical_dataæ¡ç›®
    """
    eval_gt_list = case_eval.get('ground_truth_differential', [])
    if not eval_gt_list:
        return None
    
    # æ ‡å‡†åŒ–è¯„ä¼°æ–‡ä»¶çš„GT
    eval_gt_normalized = [normalize_diagnosis(gt) for gt in eval_gt_list]
    
    best_match = None
    best_score = 0
    
    for med_item in medical_data:
        # æå–medical_dataçš„GT
        med_gt = med_item.get('GT', {})
        med_gt_list = [v for k, v in med_gt.items() if k.startswith('label_')]
        
        if not med_gt_list:
            continue
        
        # æ ‡å‡†åŒ–medical_dataçš„GT
        med_gt_normalized = [normalize_diagnosis(gt) for gt in med_gt_list]
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°
        match_count = 0
        for eval_gt in eval_gt_normalized:
            for med_gt in med_gt_normalized:
                if diagnoses_match(eval_gt, med_gt):
                    match_count += 1
                    break
        
        # è®¡ç®—åŒ¹é…ç‡
        match_score = match_count / max(len(eval_gt_normalized), len(med_gt_normalized))
        
        if match_score > best_score:
            best_score = match_score
            best_match = med_item
    
    # åªæœ‰åŒ¹é…åº¦è¶…è¿‡é˜ˆå€¼æ‰è¿”å›
    if best_score >= 0.5:
        return best_match
    
    return None


def calculate_similarity_variance(task3_pairs: List[Dict]) -> float:
    """
    è®¡ç®—task3_pairsä¸­ç›¸ä¼¼åº¦çš„æ³¢åŠ¨ç¨‹åº¦
    è¿”å›æ ‡å‡†å·®ï¼Œæ³¢åŠ¨è¶Šå¤§å€¼è¶Šé«˜
    """
    if not task3_pairs or len(task3_pairs) < 2:
        return 0.0
    
    similarities = [pair.get('similarity', 0.0) for pair in task3_pairs]
    
    # è®¡ç®—æ ‡å‡†å·®
    return float(np.std(similarities))


def calculate_similarity_range(task3_pairs: List[Dict]) -> float:
    """
    è®¡ç®—task3_pairsä¸­ç›¸ä¼¼åº¦çš„æå·®(æœ€å¤§å€¼-æœ€å°å€¼)
    """
    if not task3_pairs or len(task3_pairs) < 2:
        return 0.0
    
    similarities = [pair.get('similarity', 0.0) for pair in task3_pairs]
    return max(similarities) - min(similarities)


def get_dermlip_similarity(similarity_matrix, pred_idx: int, truth_idx: int) -> float:
    """ä»ç›¸ä¼¼åº¦çŸ©é˜µä¸­è·å–ç›¸ä¼¼åº¦åˆ†æ•°"""
    if similarity_matrix and len(similarity_matrix) > pred_idx:
        if len(similarity_matrix[pred_idx]) > truth_idx:
            return similarity_matrix[pred_idx][truth_idx]
    return 0.0


def calculate_jaccard_similarity(diag1: str, diag2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªè¯Šæ–­ä¹‹é—´çš„Jaccardç›¸ä¼¼åº¦"""
    words1 = set(normalize_diagnosis(diag1).split())
    words2 = set(normalize_diagnosis(diag2).split())
    
    if len(words1) == 0 and len(words2) == 0:
        return 0.0
    
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return intersection / union if union > 0 else 0.0


def generate_task3_pairs(
    pred_diff_list: List[str],
    truth_diff_list: List[str],
    similarity_matrix: Optional[List[List[float]]] = None
) -> List[Dict]:
    """
    ç”Ÿæˆtask3çš„ä¸¤å¯¹è¯Šæ–­ï¼Œä¼˜å…ˆé€‰æ‹©ç›¸ä¼¼åº¦å·®å¼‚å¤§çš„å¯¹
    **æ”¹è¿›ç‰ˆ**:
      - è·³è¿‡å†…å®¹é‡å¤åº¦é«˜çš„pair
      - å¯¹ç›¸ä¼¼åº¦è¿›è¡Œå½’ä¸€åŒ–(0-1)
      - ä¿ç•™åŸå§‹ç›¸ä¼¼åº¦(similarity_original)
    """
    if len(pred_diff_list) < 2 or len(truth_diff_list) < 2:
        return create_default_pairs(pred_diff_list, truth_diff_list, similarity_matrix)
    
    # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„é…å¯¹åŠå…¶ç›¸ä¼¼åº¦
    all_pairs = []
    for i, pred in enumerate(pred_diff_list):
        for j, truth in enumerate(truth_diff_list):
            # è¿‡æ»¤1: è·³è¿‡å®è´¨ç›¸åŒçš„è¯Šæ–­
            if are_diagnoses_same(pred, truth, similarity_threshold=0.85):
                continue

            # è¿‡æ»¤2: è·³è¿‡å†…å®¹é‡å¤åº¦è¿‡é«˜çš„é…å¯¹
            if has_high_content_overlap(pred, truth, overlap_threshold=0.70):
                continue

            # ç›¸ä¼¼åº¦
            if similarity_matrix and i < len(similarity_matrix) and j < len(similarity_matrix[i]):
                similarity = similarity_matrix[i][j]
            else:
                similarity = calculate_jaccard_similarity(pred, truth)
            
            all_pairs.append({
                'pred_idx': i,
                'truth_idx': j,
                'predicted': pred,
                'ground_truth': truth,
                'similarity': round(similarity, 4)
            })
    
    # è‹¥è¿‡æ»¤åé…å¯¹æ•°é‡ä¸è¶³ï¼Œå®½æ¾é‡è¯•
    if len(all_pairs) < 2:
        print(f"  è­¦å‘Š: è¿‡æ»¤ç›¸åŒ/é‡å¤è¯Šæ–­åé…å¯¹ä¸è¶³ ({len(all_pairs)}ä¸ª)ï¼Œä½¿ç”¨å®½æ¾ç­–ç•¥")
        for i, pred in enumerate(pred_diff_list):
            for j, truth in enumerate(truth_diff_list):
                if are_diagnoses_same(pred, truth, similarity_threshold=0.95):
                    continue
                if has_high_content_overlap(pred, truth, overlap_threshold=0.8):
                    continue
                if similarity_matrix and i < len(similarity_matrix) and j < len(similarity_matrix[i]):
                    similarity = similarity_matrix[i][j]
                else:
                    similarity = calculate_jaccard_similarity(pred, truth)
                all_pairs.append({
                    'pred_idx': i,
                    'truth_idx': j,
                    'predicted': pred,
                    'ground_truth': truth,
                    'similarity': round(similarity, 4)
                })
    
    # è‹¥ä»ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤å¯¹
    if len(all_pairs) < 2:
        return create_default_pairs(pred_diff_list, truth_diff_list, similarity_matrix)

    # ç›¸ä¼¼åº¦å½’ä¸€åŒ– (Min-Max)
    all_pairs = normalize_similarities_in_case(all_pairs)

    # ç­–ç•¥ï¼šé€‰æ‹©å½’ä¸€åŒ–åç›¸ä¼¼åº¦å·®å¼‚æœ€å¤§çš„ä¸¤å¯¹
    best_variance = -1
    best_pair_combo = None
    
    for i in range(len(all_pairs)):
        for j in range(i + 1, len(all_pairs)):
            pair1 = all_pairs[i]
            pair2 = all_pairs[j]
            
            if (pair1['pred_idx'] == pair2['pred_idx'] or 
                pair1['truth_idx'] == pair2['truth_idx']):
                continue
            
            sim_diff = abs(pair1['similarity'] - pair2['similarity'])
            if sim_diff > best_variance:
                best_variance = sim_diff
                best_pair_combo = (pair1, pair2)
    
    # è¾“å‡ºç»“æœ
    if best_pair_combo:
        pair1, pair2 = best_pair_combo
        return [
            {
                'pair_id': 'A',
                'predicted': pair1['predicted'],
                'ground_truth': pair1['ground_truth'],
                'similarity': round(pair1['similarity'], 4),
                'similarity_original': pair1['similarity_original']
            },
            {
                'pair_id': 'B',
                'predicted': pair2['predicted'],
                'ground_truth': pair2['ground_truth'],
                'similarity': round(pair2['similarity'], 4),
                'similarity_original': pair2['similarity_original']
            }
        ]
    else:
        random.shuffle(all_pairs)
        return [
            {
                'pair_id': 'A',
                'predicted': all_pairs[0]['predicted'],
                'ground_truth': all_pairs[0]['ground_truth'],
                'similarity': all_pairs[0]['similarity'],
                'similarity_original': all_pairs[0]['similarity_original']
            },
            {
                'pair_id': 'B',
                'predicted': all_pairs[1]['predicted'],
                'ground_truth': all_pairs[1]['ground_truth'],
                'similarity': all_pairs[1]['similarity'],
                'similarity_original': all_pairs[1]['similarity_original']
            }
        ]


def create_default_pairs(
    pred_diff_list: List[str],
    truth_diff_list: List[str],
    similarity_matrix: Optional[List[List[float]]] = None
) -> List[Dict]:
    """åˆ›å»ºé»˜è®¤çš„ä¸¤å¯¹è¯Šæ–­"""
    pairs = []
    
    for i in range(2):
        pred_idx = min(i, len(pred_diff_list) - 1) if pred_diff_list else 0
        truth_idx = min(i, len(truth_diff_list) - 1) if truth_diff_list else 0
        
        pred = pred_diff_list[pred_idx] if pred_diff_list else "æ— é¢„æµ‹è¯Šæ–­"
        truth = truth_diff_list[truth_idx] if truth_diff_list else "æ— çœŸå®è¯Šæ–­"
        
        if similarity_matrix and pred_idx < len(similarity_matrix) and truth_idx < len(similarity_matrix[pred_idx]):
            similarity = similarity_matrix[pred_idx][truth_idx]
        else:
            similarity = calculate_jaccard_similarity(pred, truth)
        
        pairs.append({
            'pair_id': chr(65 + i),
            'predicted': pred,
            'ground_truth': truth,
            'similarity': round(similarity, 4)
        })
    
    return pairs


def process_evaluation_data(
    eval_file: str,
    medical_file: str,
    output_file: str,
    use_dermlip: bool = True
) -> List[Dict]:
    """
    å¤„ç†è¯„ä¼°æ•°æ®ï¼ŒåŒ¹é…å›¾ç‰‡è·¯å¾„ï¼Œç”Ÿæˆæ ‡å‡†æ ¼å¼
    """
    print(f"è¯»å–è¯„ä¼°æ•°æ®: {eval_file}")
    with open(eval_file, 'r', encoding='utf-8') as f:
        eval_data = json.load(f)
    
    print(f"è¯»å–åŒ»ç–—æ•°æ®: {medical_file}")
    with open(medical_file, 'r', encoding='utf-8') as f:
        medical_data = json.load(f)
    
    print(f"è¯„ä¼°æ•°æ®: {len(eval_data.get('per_sample_results', []))} ä¸ªç—…ä¾‹")
    print(f"åŒ»ç–—æ•°æ®: {len(medical_data)} ä¸ªç—…ä¾‹")
    
    # ç»Ÿè®¡å˜é‡
    processed_cases = []
    matched_count = 0
    fallback_count = 0
    no_image_count = 0
    total_path_replaced = 0
    
    per_sample_results = eval_data.get('per_sample_results', [])
    
    print(f"\n{'='*70}")
    print(f"å¼€å§‹å¤„ç†ç—…ä¾‹(å…± {len(per_sample_results)} ä¸ª)...")
    print(f"{'='*70}")
    
    start_time = datetime.now()
    
    for idx, sample in enumerate(per_sample_results):
        # æ¯å¤„ç†500ä¸ªç—…ä¾‹æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if (idx + 1) % 500 == 0 or idx == 0:
            elapsed = (datetime.now() - start_time).total_seconds()
            speed = (idx + 1) / elapsed if elapsed > 0 else 0
            eta = (len(per_sample_results) - idx - 1) / speed if speed > 0 else 0
            
            print(f"\nè¿›åº¦: {idx + 1}/{len(per_sample_results)} ({(idx+1)/len(per_sample_results)*100:.1f}%)")
            print(f"  â”œâ”€ GTåŒ¹é…æˆåŠŸ: {matched_count}")
            print(f"  â”œâ”€ é¡ºåºåŒ¹é…: {fallback_count}")
            print(f"  â”œâ”€ æ— å›¾ç‰‡: {no_image_count}")
            print(f"  â”œâ”€ è·¯å¾„å·²æ›¿æ¢: {total_path_replaced}")
            print(f"  â”œâ”€ å¤„ç†é€Ÿåº¦: {speed:.1f} ä¸ª/ç§’")
            print(f"  â””â”€ é¢„è®¡å‰©ä½™æ—¶é—´: {eta/60:.1f} åˆ†é’Ÿ")
        
        case_id = sample.get('id', f'case_{idx}')
        
        # æå–é¢„æµ‹å’ŒçœŸå®çš„é‰´åˆ«è¯Šæ–­
        pred_diff = sample.get('predicted_differential', [])
        truth_diff = sample.get('ground_truth_differential', [])
        
        if not isinstance(pred_diff, list):
            pred_diff = []
        if not isinstance(truth_diff, list):
            truth_diff = []
        
        # è·å–ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = None
        if use_dermlip:
            dermlip_metrics = sample.get('dermlip_metrics', {})
            similarity_matrix = dermlip_metrics.get('similarity_matrix', None)
        
        # åŒ¹é…medical_dataä»¥è·å–image_pathså’Œprompt
        matched_item = match_case_by_gt(sample, medical_data)
        
        if matched_item:
            matched_count += 1
            image_paths = matched_item.get('image_paths', [])
            prompt = matched_item.get('prompt', '')
        else:
            # å°è¯•æŒ‰é¡ºåºåŒ¹é…(ä½œä¸ºåå¤‡æ–¹æ¡ˆ)
            fallback_count += 1
            if idx < len(medical_data):
                image_paths = medical_data[idx].get('image_paths', [])
                prompt = medical_data[idx].get('prompt', '')
            else:
                image_paths = []
                prompt = ''
        
        # ç»Ÿè®¡æ— å›¾ç‰‡çš„ç—…ä¾‹
        if not image_paths or len(image_paths) == 0:
            no_image_count += 1
        
        # æ›¿æ¢è·¯å¾„å‰ç¼€
        image_paths, replaced_count = replace_path_prefix(image_paths)
        total_path_replaced += replaced_count
        
        # ç”Ÿæˆtask3_pairsï¼šé€‰æ‹©ç›¸ä¼¼åº¦æ³¢åŠ¨å¤§çš„ä¸¤å¯¹ï¼Œä¸”ç¡®ä¿ predicted != ground_truth
        task3_pairs = generate_task3_pairs(
            pred_diff, 
            truth_diff, 
            similarity_matrix
        )
        
        # æ„å»ºç—…ä¾‹æ•°æ®
        case_data = {
            "id": case_id,
            "pmid": case_id,
            
            # ä»»åŠ¡1ï¼šå›¾ç‰‡è·¯å¾„å’Œæç¤ºè¯
            "image_paths": image_paths,
            "prompt": prompt,
            
            # ä»»åŠ¡2ï¼šè¯Šæ–­
            "predicted_diagnosis": sample.get('predicted_differential_text', ''),
            "ground_truth_diagnosis": ' | '.join(truth_diff) if truth_diff else '',
            
            # ä»»åŠ¡3ï¼šä¸¤å¯¹è¯Šæ–­åŠç›¸ä¼¼åº¦
            "task3_pairs": task3_pairs,
            
            # é¢å¤–ä¿¡æ¯
            "predicted_differential_diagnosis_full": pred_diff,
            "ground_truth_differential_diagnosis_full": truth_diff,
            
            # ç›¸ä¼¼åº¦æ³¢åŠ¨æŒ‡æ ‡
            "similarity_variance": calculate_similarity_variance(task3_pairs),
            "similarity_range": calculate_similarity_range(task3_pairs)
        }
        
        processed_cases.append(case_data)
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*70}")
    print(f"æ­¥éª¤1å¤„ç†å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"æ€»å¤„ç†ç—…ä¾‹: {len(processed_cases)}")
    print(f"GTåŒ¹é…æˆåŠŸ: {matched_count} ({matched_count/len(processed_cases)*100:.1f}%)")
    print(f"é¡ºåºåŒ¹é…: {fallback_count} ({fallback_count/len(processed_cases)*100:.1f}%)")
    print(f"æ— å›¾ç‰‡è·¯å¾„: {no_image_count} ({no_image_count/len(processed_cases)*100:.1f}%)")
    print(f"è·¯å¾„å·²æ›¿æ¢: {total_path_replaced}")
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(processed_cases, f, ensure_ascii=False, indent=2)
    
    print(f"\nä¸´æ—¶æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    return processed_cases


def get_image_signature(image_paths: List[str]) -> str:
    """
    ç”Ÿæˆå›¾ç‰‡è·¯å¾„çš„ç­¾åï¼Œç”¨äºè¯†åˆ«ç›¸åŒçš„å›¾ç‰‡é›†åˆ
    """
    if not image_paths:
        return ""
    # æå–æ–‡ä»¶åå¹¶æ’åºï¼Œç¡®ä¿ç›¸åŒå›¾ç‰‡é›†åˆç”Ÿæˆç›¸åŒç­¾å
    sorted_paths = sorted([os.path.basename(p) for p in image_paths])
    return "|".join(sorted_paths)


def validate_and_filter_data(
    input_file: str,
    output_file: str,
    max_cases: int = 50,
    min_similarity_variance: float = 0.0,
    random_selection: bool = True,
    random_seed: int = 42
) -> Tuple[List[Dict], List[Dict]]:
    """
    éªŒè¯å¹¶ç­›é€‰æ•°æ®ï¼Œä¼˜å…ˆä¿ç•™ç›¸ä¼¼åº¦æ³¢åŠ¨å¤§çš„ç—…ä¾‹
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        max_cases: æœ€å¤§ä¿ç•™ç—…ä¾‹æ•°
        min_similarity_variance: æœ€å°ç›¸ä¼¼åº¦æ³¢åŠ¨é˜ˆå€¼
        random_selection: æ˜¯å¦éšæœºé€‰æ‹©ï¼ˆè€Œéè¿ç»­é€‰æ‹©ï¼‰
        random_seed: éšæœºç§å­
    """
    print(f"\n{'='*70}")
    print("æ­¥éª¤2: å¼€å§‹æ•°æ®ç­›é€‰å’ŒéªŒè¯")
    print(f"{'='*70}")
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    if random_selection:
        random.seed(random_seed)
        np.random.seed(random_seed)
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"è¯»å–ä¸´æ—¶æ•°æ®: {len(data)} ä¸ªç—…ä¾‹")
    print(f"éšæœºé€‰æ‹©æ¨¡å¼: {'å¼€å¯' if random_selection else 'å…³é—­'}")
    print(f"éšæœºç§å­: {random_seed if random_selection else 'N/A'}")
    
    valid_cases = []
    removed_cases = []
    
    # ç”¨äºè·Ÿè¸ªå·²ä½¿ç”¨çš„å›¾ç‰‡é›†åˆ
    used_image_signatures = set()
    
    # ç»Ÿè®¡å„ç§ç§»é™¤åŸå› 
    reason_stats = {
        'åŒ…å«Noneå€¼': 0,
        'task3_pairsé—®é¢˜': 0,
        'è¯Šæ–­ä¸ºç©º': 0,
        'è¯Šæ–­ä¸€è‡´': 0,
        'æ— å›¾ç‰‡è·¯å¾„': 0,
        'ç¼ºå°‘å¿…éœ€å­—æ®µ': 0,
        'ç›¸ä¼¼åº¦æ³¢åŠ¨è¿‡å°': 0,
        'pairä¸­predictedå’Œtruthç›¸åŒ': 0,
        'å›¾ç‰‡é‡å¤': 0  # æ–°å¢
    }
    
    print(f"\nå¼€å§‹éªŒè¯...")
    
    for idx, case in enumerate(data):
        if (idx + 1) % 1000 == 0:
            print(f"  éªŒè¯è¿›åº¦: {idx + 1}/{len(data)} ({(idx+1)/len(data)*100:.1f}%)")
        
        case_id = case.get('pmid', case.get('id', f'case_{idx}'))
        remove_reasons = []
        
        # è§„åˆ™1: æ£€æŸ¥æ˜¯å¦æœ‰Noneå€¼
        if has_none_values(case):
            remove_reasons.append("åŒ…å«Noneå€¼")
            reason_stats['åŒ…å«Noneå€¼'] += 1
        
        # è§„åˆ™2: æ£€æŸ¥task3_pairs
        task3_valid, task3_reason = validate_task3_pairs(case.get('task3_pairs', []))
        if not task3_valid:
            remove_reasons.append(task3_reason)
            reason_stats['task3_pairsé—®é¢˜'] += 1
        
        # è§„åˆ™3: æ£€æŸ¥ task3_pairs ä¸­æ˜¯å¦æœ‰ predicted å’Œ ground_truth ç›¸åŒçš„æƒ…å†µ
        task3_pairs = case.get('task3_pairs', [])
        if task3_pairs:
            for pair in task3_pairs:
                pred = pair.get('predicted', '')
                truth = pair.get('ground_truth', '')
                if are_diagnoses_same(pred, truth, similarity_threshold=0.85):
                    remove_reasons.append(f"Pair {pair.get('pair_id')}ä¸­predictedå’Œtruthå®è´¨ç›¸åŒ")
                    reason_stats['pairä¸­predictedå’Œtruthç›¸åŒ'] += 1
                    break
        
        # è§„åˆ™4: æ£€æŸ¥è¯Šæ–­æ˜¯å¦ä¸€è‡´
        pred_diag = case.get('predicted_diagnosis', '')
        truth_diag = case.get('ground_truth_diagnosis', '')
        
        if not pred_diag or not truth_diag:
            remove_reasons.append("è¯Šæ–­ä¸ºç©º")
            reason_stats['è¯Šæ–­ä¸ºç©º'] += 1
        elif diagnoses_are_same(pred_diag, truth_diag):
            remove_reasons.append(f"è¯Šæ–­å®Œå…¨ä¸€è‡´")
            reason_stats['è¯Šæ–­ä¸€è‡´'] += 1
        
        # è§„åˆ™5: æ£€æŸ¥å›¾ç‰‡è·¯å¾„
        image_paths = case.get('image_paths', [])
        if not image_paths or len(image_paths) == 0:
            remove_reasons.append("æ— å›¾ç‰‡è·¯å¾„")
            reason_stats['æ— å›¾ç‰‡è·¯å¾„'] += 1
        
        # è§„åˆ™6: æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['id', 'pmid', 'prompt']
        for field in required_fields:
            if not case.get(field):
                remove_reasons.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                reason_stats['ç¼ºå°‘å¿…éœ€å­—æ®µ'] += 1
                break
        
        # è§„åˆ™7: æ£€æŸ¥ç›¸ä¼¼åº¦æ³¢åŠ¨
        sim_variance = case.get('similarity_variance', 0.0)
        if sim_variance < min_similarity_variance:
            remove_reasons.append(f"ç›¸ä¼¼åº¦æ³¢åŠ¨è¿‡å°: {sim_variance:.4f} < {min_similarity_variance}")
            reason_stats['ç›¸ä¼¼åº¦æ³¢åŠ¨è¿‡å°'] += 1
        
        # è§„åˆ™8: æ£€æŸ¥å›¾ç‰‡æ˜¯å¦é‡å¤
        if image_paths:
            img_signature = get_image_signature(image_paths)
            if img_signature in used_image_signatures:
                remove_reasons.append(f"å›¾ç‰‡é›†åˆé‡å¤")
                reason_stats['å›¾ç‰‡é‡å¤'] += 1
        
        if remove_reasons:
            removed_cases.append({
                'case_id': case_id,
                'reasons': remove_reasons,
                'predicted_diagnosis': pred_diag,
                'ground_truth_diagnosis': truth_diag,
                'similarity_variance': case.get('similarity_variance', 0.0),
                'task3_pairs': task3_pairs,
                'image_signature': get_image_signature(image_paths) if image_paths else ''
            })
        else:
            # è®°å½•å›¾ç‰‡ç­¾åï¼Œé¿å…é‡å¤
            if image_paths:
                img_signature = get_image_signature(image_paths)
                used_image_signatures.add(img_signature)
            valid_cases.append(case)
    
    print(f"\nåŸºæœ¬éªŒè¯å®Œæˆ:")
    print(f"  é€šè¿‡éªŒè¯: {len(valid_cases)} ä¸ª")
    print(f"  æœªé€šè¿‡éªŒè¯: {len(removed_cases)} ä¸ª")
    
    print(f"\næœªé€šè¿‡åŸå› ç»Ÿè®¡:")
    for reason, count in sorted(reason_stats.items(), key=lambda x: x[1], reverse=True):
        if count > 0:
            print(f"  â€¢ {reason}: {count} ä¸ª")
    
    # æŒ‰ç›¸ä¼¼åº¦æ³¢åŠ¨æ’åºï¼Œé€‰æ‹©æ³¢åŠ¨æœ€å¤§çš„ç—…ä¾‹
    if len(valid_cases) > max_cases:
        print(f"\n{'='*70}")
        print(f"æœ‰æ•ˆç—…ä¾‹æ•°({len(valid_cases)})è¶…è¿‡ç›®æ ‡æ•°({max_cases})ï¼Œå¼€å§‹ç­›é€‰...")
        print(f"{'='*70}")
        
        # æŒ‰ç›¸ä¼¼åº¦æ³¢åŠ¨(æ ‡å‡†å·®å’Œæå·®çš„ç»„åˆ)é™åºæ’åº
        valid_cases.sort(
            key=lambda x: (
                x.get('similarity_variance', 0) * 0.6 + 
                x.get('similarity_range', 0) * 0.4
            ),
            reverse=True
        )
        
        # æ˜¾ç¤ºæ³¢åŠ¨åˆ†å¸ƒ
        print(f"\nç›¸ä¼¼åº¦æ³¢åŠ¨åˆ†æ:")
        top_10 = [round(c.get('similarity_variance', 0), 4) for c in valid_cases[:10]]
        print(f"  å‰10ä¸ªç—…ä¾‹çš„æ–¹å·®: {top_10}")
        if len(valid_cases) >= 10:
            bottom_10 = [round(c.get('similarity_variance', 0), 4) for c in valid_cases[-10:]]
            print(f"  å10ä¸ªç—…ä¾‹çš„æ–¹å·®: {bottom_10}")
        
        if random_selection:
            # éšæœºé€‰æ‹©æ¨¡å¼ï¼šä»é«˜æ³¢åŠ¨ç—…ä¾‹ä¸­åˆ†å±‚éšæœºé‡‡æ ·
            print(f"\nä½¿ç”¨éšæœºé€‰æ‹©æ¨¡å¼ (ç§å­={random_seed}):")
            
            # å°†ç—…ä¾‹åˆ†æˆé«˜ã€ä¸­ã€ä½æ³¢åŠ¨ä¸‰æ¡£
            variances = [c.get('similarity_variance', 0) for c in valid_cases]
            high_threshold = np.percentile(variances, 75)
            medium_threshold = np.percentile(variances, 50)
            
            high_variance_cases = [c for c in valid_cases 
                                  if c.get('similarity_variance', 0) >= high_threshold]
            medium_variance_cases = [c for c in valid_cases 
                                    if medium_threshold <= c.get('similarity_variance', 0) < high_threshold]
            low_variance_cases = [c for c in valid_cases 
                                 if c.get('similarity_variance', 0) < medium_threshold]
            
            print(f"  é«˜æ³¢åŠ¨ç—…ä¾‹: {len(high_variance_cases)} ä¸ª (>= {high_threshold:.4f})")
            print(f"  ä¸­æ³¢åŠ¨ç—…ä¾‹: {len(medium_variance_cases)} ä¸ª (>= {medium_threshold:.4f})")
            print(f"  ä½æ³¢åŠ¨ç—…ä¾‹: {len(low_variance_cases)} ä¸ª")
            
            # åˆ†é…ç­–ç•¥ï¼š70%é«˜æ³¢åŠ¨ï¼Œ25%ä¸­æ³¢åŠ¨ï¼Œ5%ä½æ³¢åŠ¨
            n_high = min(int(max_cases * 0.70), len(high_variance_cases))
            n_medium = min(int(max_cases * 0.25), len(medium_variance_cases))
            n_low = max_cases - n_high - n_medium
            n_low = min(n_low, len(low_variance_cases))
            
            # å¦‚æœæŸä¸€æ¡£ä¸å¤Ÿï¼Œä»å…¶ä»–æ¡£è¡¥å……
            total_selected = n_high + n_medium + n_low
            if total_selected < max_cases:
                remaining = max_cases - total_selected
                if len(high_variance_cases) > n_high:
                    additional = min(remaining, len(high_variance_cases) - n_high)
                    n_high += additional
                    remaining -= additional
                if remaining > 0 and len(medium_variance_cases) > n_medium:
                    additional = min(remaining, len(medium_variance_cases) - n_medium)
                    n_medium += additional
                    remaining -= additional
                if remaining > 0 and len(low_variance_cases) > n_low:
                    n_low += min(remaining, len(low_variance_cases) - n_low)
            
            print(f"\né€‰æ‹©ç­–ç•¥:")
            print(f"  ä»é«˜æ³¢åŠ¨ä¸­é€‰æ‹©: {n_high} ä¸ª")
            print(f"  ä»ä¸­æ³¢åŠ¨ä¸­é€‰æ‹©: {n_medium} ä¸ª")
            print(f"  ä»ä½æ³¢åŠ¨ä¸­é€‰æ‹©: {n_low} ä¸ª")
            
            # éšæœºé‡‡æ ·
            selected_cases = []
            if n_high > 0 and len(high_variance_cases) > 0:
                selected_cases.extend(random.sample(high_variance_cases, n_high))
            if n_medium > 0 and len(medium_variance_cases) > 0:
                selected_cases.extend(random.sample(medium_variance_cases, n_medium))
            if n_low > 0 and len(low_variance_cases) > 0:
                selected_cases.extend(random.sample(low_variance_cases, n_low))
            
            # éšæœºæ‰“ä¹±é¡ºåºï¼Œç¡®ä¿ä¸ç›¸é‚»
            random.shuffle(selected_cases)
            
            print(f"\nå®é™…é€‰æ‹©: {len(selected_cases)} ä¸ªç—…ä¾‹")
            
            # æ‰¾å‡ºæœªè¢«é€‰ä¸­çš„ç—…ä¾‹
            selected_ids = set(c.get('id') for c in selected_cases)
            extra_cases = [c for c in valid_cases if c.get('id') not in selected_ids]
            
            valid_cases = selected_cases
        else:
            # é¡ºåºé€‰æ‹©æ¨¡å¼ï¼šç›´æ¥ä¿ç•™å‰max_casesä¸ª
            print(f"\nä½¿ç”¨é¡ºåºé€‰æ‹©æ¨¡å¼:")
            extra_cases = valid_cases[max_cases:]
            valid_cases = valid_cases[:max_cases]
        
        print(f"\næœ€ç»ˆä¿ç•™: {len(valid_cases)} ä¸ªç—…ä¾‹")
        print(f"é¢å¤–ç§»é™¤: {len(extra_cases)} ä¸ªç—…ä¾‹")
        
        # å°†å¤šä½™çš„ç—…ä¾‹åŠ å…¥ç§»é™¤åˆ—è¡¨
        for case in extra_cases:
            removed_cases.append({
                'case_id': case.get('pmid', case.get('id')),
                'reasons': [f'æœªè¢«é€‰å…¥æœ€ç»ˆçš„{max_cases}ä¸ªç—…ä¾‹'],
                'predicted_diagnosis': case.get('predicted_diagnosis', ''),
                'ground_truth_diagnosis': case.get('ground_truth_diagnosis', ''),
                'similarity_variance': case.get('similarity_variance', 0.0)
            })
    
    # ä¿å­˜æœ‰æ•ˆæ•°æ®
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(valid_cases, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*70}")
    print("ç­›é€‰å®Œæˆï¼šæœ€ç»ˆç»Ÿè®¡")
    print(f"{'='*70}")
    print(f"åŸå§‹ç—…ä¾‹æ•°: {len(data)}")
    print(f"âœ“ æœ€ç»ˆä¿ç•™: {len(valid_cases)} ä¸ª")
    print(f"âœ— ç§»é™¤æ€»æ•°: {len(removed_cases)} ä¸ª")
    print(f"ä¿ç•™æ¯”ä¾‹: {len(valid_cases)/len(data)*100:.2f}%")
    
    # ç»Ÿè®¡æœ€ç»ˆä¿ç•™ç—…ä¾‹çš„ç›¸ä¼¼åº¦æ³¢åŠ¨
    if valid_cases:
        variances = [c.get('similarity_variance', 0) for c in valid_cases]
        ranges = [c.get('similarity_range', 0) for c in valid_cases]
        
        print(f"\nä¿ç•™ç—…ä¾‹çš„ç›¸ä¼¼åº¦æ³¢åŠ¨ç»Ÿè®¡:")
        print(f"  æ–¹å·®ç»Ÿè®¡:")
        print(f"    å¹³å‡å€¼: {np.mean(variances):.4f}")
        print(f"    ä¸­ä½æ•°: {np.median(variances):.4f}")
        print(f"    æœ€å¤§å€¼: {np.max(variances):.4f}")
        print(f"    æœ€å°å€¼: {np.min(variances):.4f}")
        print(f"  æå·®ç»Ÿè®¡:")
        print(f"    å¹³å‡å€¼: {np.mean(ranges):.4f}")
        print(f"    ä¸­ä½æ•°: {np.median(ranges):.4f}")
        print(f"    æœ€å¤§å€¼: {np.max(ranges):.4f}")
        print(f"    æœ€å°å€¼: {np.min(ranges):.4f}")
    
    print(f"\nâœ“ æœ€ç»ˆæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    print(f"{'='*70}\n")
    
    return valid_cases, removed_cases


def has_none_values(obj, path="") -> bool:
    """é€’å½’æ£€æŸ¥å¯¹è±¡ä¸­æ˜¯å¦æœ‰Noneå€¼"""
    if obj is None:
        return True
    
    if isinstance(obj, dict):
        for key, value in obj.items():
            if has_none_values(value, f"{path}.{key}"):
                return True
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            if has_none_values(item, f"{path}[{i}]"):
                return True
    
    return False


def validate_task3_pairs(pairs: List[Dict]) -> Tuple[bool, str]:
    """éªŒè¯task3_pairs"""
    if not pairs or len(pairs) != 2:
        return False, f"task3_pairsæ•°é‡å¿…é¡»ä¸º2ä¸ª (å½“å‰: {len(pairs) if pairs else 0})"
    
    for i, pair in enumerate(pairs):
        if not isinstance(pair, dict):
            return False, f"task3_pairs[{i}]ä¸æ˜¯å­—å…¸"
        
        required_fields = ['pair_id', 'predicted', 'ground_truth', 'similarity']
        for field in required_fields:
            if field not in pair:
                return False, f"task3_pairs[{i}]ç¼ºå°‘å­—æ®µ: {field}"
            if pair[field] is None:
                return False, f"task3_pairs[{i}].{field}ä¸ºNone"
    
    pair_a, pair_b = pairs[0], pairs[1]
    
    if pair_a.get('pair_id') == pair_b.get('pair_id'):
        return False, "ä¸¤å¯¹çš„pair_idç›¸åŒ"
    
    if (pair_a.get('predicted') == pair_b.get('predicted') and 
        pair_a.get('ground_truth') == pair_b.get('ground_truth')):
        return False, "ä¸¤å¯¹çš„è¯Šæ–­å†…å®¹å®Œå…¨ç›¸åŒ"
    
    return True, ""


def diagnoses_are_same(diag1: str, diag2: str) -> bool:
    """åˆ¤æ–­ä¸¤ä¸ªè¯Šæ–­æ˜¯å¦ç›¸åŒ"""
    if not diag1 or not diag2:
        return False
    
    d1 = normalize_diagnosis(diag1)
    d2 = normalize_diagnosis(diag2)
    
    if d1 == d2:
        return True
    
    # æ£€æŸ¥åŒ…å«å…³ç³»
    if d1 in d2 or d2 in d1:
        len_ratio = min(len(d1), len(d2)) / max(len(d1), len(d2))
        if len_ratio > 0.8:
            return True
    
    return False


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print(" " * 20 + "åŒ»ç–—æ•°æ®å¤„ç†å’Œç­›é€‰è„šæœ¬")
    print("="*70)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    eval_file = "scin_dd_evaluation.json"
    medical_file = "SCIN_DD_input.json"
    temp_output = "data_temp.json"
    final_output = "data_filtered.json"
    
    # æ­¥éª¤1: å¤„ç†å’ŒåŒ¹é…æ•°æ®
    print("\nğŸ“‹ æ­¥éª¤1: å¤„ç†è¯„ä¼°æ•°æ®å¹¶åŒ¹é…å›¾ç‰‡è·¯å¾„")
    print("-" * 70)
    try:
        processed_data = process_evaluation_data(
            eval_file=eval_file,
            medical_file=medical_file,
            output_file=temp_output,
            use_dermlip=True
        )
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: æ­¥éª¤1å¤„ç†å¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return
    
    # æ­¥éª¤2: ç­›é€‰æ•°æ®
    print("\nğŸ“Š æ­¥éª¤2: ç­›é€‰æ•°æ®(ä¼˜å…ˆä¿ç•™ç›¸ä¼¼åº¦æ³¢åŠ¨å¤§çš„ç—…ä¾‹)")
    print("-" * 70)
    try:
        valid_cases, removed_cases = validate_and_filter_data(
            input_file=temp_output,
            output_file=final_output,
            max_cases=50,
            min_similarity_variance=0.0,
            random_selection=True,  # å¼€å¯éšæœºé€‰æ‹©
            random_seed=42  # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤
        )
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: æ­¥éª¤2ç­›é€‰å¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if valid_cases:
        print("="*70)
        print("âœ“ å¤„ç†å®Œæˆï¼ä»¥ä¸‹æ˜¯ç¬¬ä¸€ä¸ªç—…ä¾‹ç¤ºä¾‹:")
        print("="*70)
        example = valid_cases[0]
        print(f"\nç—…ä¾‹ID: {example.get('id')}")
        print(f"å›¾ç‰‡æ•°é‡: {len(example.get('image_paths', []))}")
        if example.get('image_paths'):
            print(f"å›¾ç‰‡è·¯å¾„ç¤ºä¾‹: {example.get('image_paths')[0]}")
        print(f"é¢„æµ‹è¯Šæ–­: {example.get('predicted_diagnosis', '')[:80]}...")
        print(f"çœŸå®è¯Šæ–­: {example.get('ground_truth_diagnosis', '')[:80]}...")
        print(f"ç›¸ä¼¼åº¦æ–¹å·®: {example.get('similarity_variance', 0):.4f}")
        print(f"ç›¸ä¼¼åº¦æå·®: {example.get('similarity_range', 0):.4f}")
        print("\nTask3é…å¯¹:")
        for pair in example.get('task3_pairs', []):
            print(f"  {pair['pair_id']}å¯¹:")
            print(f"    é¢„æµ‹: {pair['predicted']}")
            print(f"    çœŸå®: {pair['ground_truth']}")
            print(f"    ç›¸ä¼¼åº¦: {pair['similarity']}")
            # éªŒè¯æ˜¯å¦ç›¸åŒ
            if are_diagnoses_same(pair['predicted'], pair['ground_truth'], 0.85):
                print(f"    âš ï¸ è­¦å‘Š: è¯¥å¯¹ä¸­predictedå’Œtruthå®è´¨ç›¸åŒ!")
    
    # æ˜¾ç¤ºä¸€äº›è¢«ç§»é™¤çš„æ¡ˆä¾‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    if removed_cases:
        print("\n" + "="*70)
        print("ç§»é™¤æ¡ˆä¾‹ç¤ºä¾‹(å‰3ä¸ª):")
        print("="*70)
        for i, case in enumerate(removed_cases[:3]):
            print(f"\n{i+1}. ç—…ä¾‹ID: {case['case_id']}")
            print(f"   ç§»é™¤åŸå› : {', '.join(case['reasons'])}")
            if 'task3_pairs' in case and case['task3_pairs']:
                print(f"   Task3é…å¯¹è¯¦æƒ…:")
                for pair in case['task3_pairs']:
                    pred = pair.get('predicted', '')
                    truth = pair.get('ground_truth', '')
                    same = are_diagnoses_same(pred, truth, 0.85)
                    print(f"     {pair.get('pair_id')}å¯¹: pred='{pred[:40]}...' truth='{truth[:40]}...' ç›¸åŒ={same}")
    
    print("\n" + "="*70)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    print("\nâœ“ æ‰€æœ‰å¤„ç†å®Œæˆï¼")
    print(f"âœ“ æœ€ç»ˆæ•°æ®ä¿å­˜åœ¨: {final_output}")
    print(f"âœ“ ä¸´æ—¶æ•°æ®ä¿å­˜åœ¨: {temp_output}")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()